{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Email Dataset\n",
    "## Initial Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/enron/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFpr\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import cross_validation\n",
    "sys.path.append(\"../tools/\")\n",
    "%matplotlib inline\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_email = ['to_messages', 'from_messages',  'from_poi_to_this_person',\n",
    "           'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
    "# finance data\n",
    "features_finance = ['salary', 'bonus', 'long_term_incentive', 'deferred_income',\n",
    "             'deferral_payments', 'loan_advances', 'other', 'expenses',\n",
    "             'director_fees', 'total_payments',\n",
    "             'exercised_stock_options', 'restricted_stock',\n",
    "             'restricted_stock_deferred', 'total_stock_value']\n",
    "# all features\n",
    "features_list = features_email + features_finance\n",
    "# all features column names\n",
    "features_column_names = ['poi'] + ['email_address'] + features_email + features_finance\n",
    "# all features data type\n",
    "features_dtype = [bool] + [str] + list(np.repeat(float, 19))\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# converting the data into a data frame\n",
    "df = DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "# reordering the columns\n",
    "df = df.loc[:, features_column_names]\n",
    "\n",
    "# converting the data type\n",
    "for i in xrange(len(features_column_names)):\n",
    "    df[features_column_names[i]] = df[features_column_names[i]].astype(features_dtype[i], errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Questions\n",
    "### Size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset Shape:\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the dataset shows that there are 144 rows (persons) and 24 columns (features) of which 3 are the interaction with POS ratios that were just added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can I define a question? (given in the project)\n",
    "##### Question No. 1\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "##### Question No. 2\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "##### Question No. 3\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "##### Question No. 4\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "##### Question No. 5\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "\n",
    "##### Question No. 6\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "### 1. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier investigation\n",
    "In the follwing the dataset will be investigated for outliers & extreme values. Outliers are defined as values above Q3 + 1.5 IQR, extreme values as those above Q3 + 3 IQR. Those values that can be found will be checked manually to make sure they are not mistakes that happened during the import. Later some outliers will be removed to test if this increases the accuracy of the algorithm. Given that most of the POIs had extremely high salary and bonus payments, removing those outliers might acutally hurt the outcome of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>8.200000e+01</td>\n",
       "      <td>6.600000e+01</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>3.900000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>9.500000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.250000e+02</td>\n",
       "      <td>1.020000e+02</td>\n",
       "      <td>1.100000e+02</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>1.260000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2073.860465</td>\n",
       "      <td>608.790698</td>\n",
       "      <td>64.895349</td>\n",
       "      <td>41.232558</td>\n",
       "      <td>1176.465116</td>\n",
       "      <td>5.621943e+05</td>\n",
       "      <td>2.374235e+06</td>\n",
       "      <td>1.470361e+06</td>\n",
       "      <td>-1.140475e+06</td>\n",
       "      <td>1.642674e+06</td>\n",
       "      <td>4.196250e+07</td>\n",
       "      <td>9.190650e+05</td>\n",
       "      <td>1.087289e+05</td>\n",
       "      <td>1.668049e+05</td>\n",
       "      <td>5.081526e+06</td>\n",
       "      <td>5.987054e+06</td>\n",
       "      <td>2.321741e+06</td>\n",
       "      <td>1.664106e+05</td>\n",
       "      <td>6.773957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2582.700981</td>\n",
       "      <td>1841.033949</td>\n",
       "      <td>86.979244</td>\n",
       "      <td>100.073111</td>\n",
       "      <td>1178.317641</td>\n",
       "      <td>2.716369e+06</td>\n",
       "      <td>1.071333e+07</td>\n",
       "      <td>5.942759e+06</td>\n",
       "      <td>4.025406e+06</td>\n",
       "      <td>5.161930e+06</td>\n",
       "      <td>4.708321e+07</td>\n",
       "      <td>4.589253e+06</td>\n",
       "      <td>5.335348e+05</td>\n",
       "      <td>3.198914e+05</td>\n",
       "      <td>2.906172e+07</td>\n",
       "      <td>3.106201e+07</td>\n",
       "      <td>1.251828e+07</td>\n",
       "      <td>4.201494e+06</td>\n",
       "      <td>3.895777e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.770000e+02</td>\n",
       "      <td>7.000000e+04</td>\n",
       "      <td>6.922300e+04</td>\n",
       "      <td>-2.799289e+07</td>\n",
       "      <td>-1.025000e+05</td>\n",
       "      <td>4.000000e+05</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>3.285000e+03</td>\n",
       "      <td>1.480000e+02</td>\n",
       "      <td>3.285000e+03</td>\n",
       "      <td>-2.604490e+06</td>\n",
       "      <td>-7.576788e+06</td>\n",
       "      <td>-4.409300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>541.250000</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>249.750000</td>\n",
       "      <td>2.118160e+05</td>\n",
       "      <td>4.312500e+05</td>\n",
       "      <td>2.812500e+05</td>\n",
       "      <td>-6.948620e+05</td>\n",
       "      <td>8.157300e+04</td>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.215000e+03</td>\n",
       "      <td>2.261400e+04</td>\n",
       "      <td>9.878400e+04</td>\n",
       "      <td>3.944750e+05</td>\n",
       "      <td>5.278862e+05</td>\n",
       "      <td>2.540180e+05</td>\n",
       "      <td>-3.896218e+05</td>\n",
       "      <td>4.945102e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1211.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>740.500000</td>\n",
       "      <td>2.599960e+05</td>\n",
       "      <td>7.693750e+05</td>\n",
       "      <td>4.420350e+05</td>\n",
       "      <td>-1.597920e+05</td>\n",
       "      <td>2.274490e+05</td>\n",
       "      <td>4.176250e+07</td>\n",
       "      <td>5.238200e+04</td>\n",
       "      <td>4.695000e+04</td>\n",
       "      <td>1.085790e+05</td>\n",
       "      <td>1.101393e+06</td>\n",
       "      <td>1.310814e+06</td>\n",
       "      <td>4.517400e+05</td>\n",
       "      <td>-1.469750e+05</td>\n",
       "      <td>1.102872e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2634.750000</td>\n",
       "      <td>145.500000</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>24.750000</td>\n",
       "      <td>1888.250000</td>\n",
       "      <td>3.121170e+05</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>9.386720e+05</td>\n",
       "      <td>-3.834600e+04</td>\n",
       "      <td>1.002672e+06</td>\n",
       "      <td>8.212500e+07</td>\n",
       "      <td>3.620960e+05</td>\n",
       "      <td>7.995250e+04</td>\n",
       "      <td>1.137840e+05</td>\n",
       "      <td>2.093263e+06</td>\n",
       "      <td>2.547724e+06</td>\n",
       "      <td>1.002370e+06</td>\n",
       "      <td>-7.500975e+04</td>\n",
       "      <td>2.949847e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15149.000000</td>\n",
       "      <td>14368.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>2.670423e+07</td>\n",
       "      <td>9.734362e+07</td>\n",
       "      <td>4.852193e+07</td>\n",
       "      <td>-8.330000e+02</td>\n",
       "      <td>3.208340e+07</td>\n",
       "      <td>8.392500e+07</td>\n",
       "      <td>4.266759e+07</td>\n",
       "      <td>5.235198e+06</td>\n",
       "      <td>1.398517e+06</td>\n",
       "      <td>3.098866e+08</td>\n",
       "      <td>3.117640e+08</td>\n",
       "      <td>1.303223e+08</td>\n",
       "      <td>1.545629e+07</td>\n",
       "      <td>4.345095e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        to_messages  from_messages  from_poi_to_this_person  \\\n",
       "count     86.000000      86.000000                86.000000   \n",
       "mean    2073.860465     608.790698                64.895349   \n",
       "std     2582.700981    1841.033949                86.979244   \n",
       "min       57.000000      12.000000                 0.000000   \n",
       "25%      541.250000      22.750000                10.000000   \n",
       "50%     1211.000000      41.000000                35.000000   \n",
       "75%     2634.750000     145.500000                72.250000   \n",
       "max    15149.000000   14368.000000               528.000000   \n",
       "\n",
       "       from_this_person_to_poi  shared_receipt_with_poi        salary  \\\n",
       "count                86.000000                86.000000  9.500000e+01   \n",
       "mean                 41.232558              1176.465116  5.621943e+05   \n",
       "std                 100.073111              1178.317641  2.716369e+06   \n",
       "min                   0.000000                 2.000000  4.770000e+02   \n",
       "25%                   1.000000               249.750000  2.118160e+05   \n",
       "50%                   8.000000               740.500000  2.599960e+05   \n",
       "75%                  24.750000              1888.250000  3.121170e+05   \n",
       "max                 609.000000              5521.000000  2.670423e+07   \n",
       "\n",
       "              bonus  long_term_incentive  deferred_income  deferral_payments  \\\n",
       "count  8.200000e+01         6.600000e+01     4.900000e+01       3.900000e+01   \n",
       "mean   2.374235e+06         1.470361e+06    -1.140475e+06       1.642674e+06   \n",
       "std    1.071333e+07         5.942759e+06     4.025406e+06       5.161930e+06   \n",
       "min    7.000000e+04         6.922300e+04    -2.799289e+07      -1.025000e+05   \n",
       "25%    4.312500e+05         2.812500e+05    -6.948620e+05       8.157300e+04   \n",
       "50%    7.693750e+05         4.420350e+05    -1.597920e+05       2.274490e+05   \n",
       "75%    1.200000e+06         9.386720e+05    -3.834600e+04       1.002672e+06   \n",
       "max    9.734362e+07         4.852193e+07    -8.330000e+02       3.208340e+07   \n",
       "\n",
       "       loan_advances         other      expenses  director_fees  \\\n",
       "count   4.000000e+00  9.300000e+01  9.500000e+01   1.700000e+01   \n",
       "mean    4.196250e+07  9.190650e+05  1.087289e+05   1.668049e+05   \n",
       "std     4.708321e+07  4.589253e+06  5.335348e+05   3.198914e+05   \n",
       "min     4.000000e+05  2.000000e+00  1.480000e+02   3.285000e+03   \n",
       "25%     1.600000e+06  1.215000e+03  2.261400e+04   9.878400e+04   \n",
       "50%     4.176250e+07  5.238200e+04  4.695000e+04   1.085790e+05   \n",
       "75%     8.212500e+07  3.620960e+05  7.995250e+04   1.137840e+05   \n",
       "max     8.392500e+07  4.266759e+07  5.235198e+06   1.398517e+06   \n",
       "\n",
       "       total_payments  exercised_stock_options  restricted_stock  \\\n",
       "count    1.250000e+02             1.020000e+02      1.100000e+02   \n",
       "mean     5.081526e+06             5.987054e+06      2.321741e+06   \n",
       "std      2.906172e+07             3.106201e+07      1.251828e+07   \n",
       "min      1.480000e+02             3.285000e+03     -2.604490e+06   \n",
       "25%      3.944750e+05             5.278862e+05      2.540180e+05   \n",
       "50%      1.101393e+06             1.310814e+06      4.517400e+05   \n",
       "75%      2.093263e+06             2.547724e+06      1.002370e+06   \n",
       "max      3.098866e+08             3.117640e+08      1.303223e+08   \n",
       "\n",
       "       restricted_stock_deferred  total_stock_value  \n",
       "count               1.800000e+01       1.260000e+02  \n",
       "mean                1.664106e+05       6.773957e+06  \n",
       "std                 4.201494e+06       3.895777e+07  \n",
       "min                -7.576788e+06      -4.409300e+04  \n",
       "25%                -3.896218e+05       4.945102e+05  \n",
       "50%                -1.469750e+05       1.102872e+06  \n",
       "75%                -7.500975e+04       2.949847e+06  \n",
       "max                 1.545629e+07       4.345095e+08  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change for each feature to avoid repetitive code blocks\n",
    "current_feature = 'restricted_stock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 107.,    2.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    1.]),\n",
       " array([ -2.60449000e+06,   1.06881889e+07,   2.39808678e+07,\n",
       "          3.72735467e+07,   5.05662256e+07,   6.38589045e+07,\n",
       "          7.71515834e+07,   9.04442623e+07,   1.03736941e+08,\n",
       "          1.17029620e+08,   1.30322299e+08]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEJCAYAAACaFuz/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqXGGXpIaZ+glqXGGXpIaZ+glqXH/D41BtN7LQCB+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aa74890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[current_feature].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poi</th>\n",
       "      <th>email_address</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>salary</th>\n",
       "      <th>bonus</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>...</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>other</th>\n",
       "      <th>expenses</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BHATNAGAR SANJAY</th>\n",
       "      <td>False</td>\n",
       "      <td>sanjay.bhatnagar@enron.com</td>\n",
       "      <td>523.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>137864.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>2604490.0</td>\n",
       "      <td>-2604490.0</td>\n",
       "      <td>15456290.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    poi               email_address  to_messages  \\\n",
       "BHATNAGAR SANJAY  False  sanjay.bhatnagar@enron.com        523.0   \n",
       "\n",
       "                  from_messages  from_poi_to_this_person  \\\n",
       "BHATNAGAR SANJAY           29.0                      0.0   \n",
       "\n",
       "                  from_this_person_to_poi  shared_receipt_with_poi  salary  \\\n",
       "BHATNAGAR SANJAY                      1.0                    463.0     NaN   \n",
       "\n",
       "                  bonus  long_term_incentive        ...          \\\n",
       "BHATNAGAR SANJAY    NaN                  NaN        ...           \n",
       "\n",
       "                  deferral_payments  loan_advances     other  expenses  \\\n",
       "BHATNAGAR SANJAY                NaN            NaN  137864.0       NaN   \n",
       "\n",
       "                  director_fees  total_payments  exercised_stock_options  \\\n",
       "BHATNAGAR SANJAY       137864.0      15456290.0                2604490.0   \n",
       "\n",
       "                  restricted_stock  restricted_stock_deferred  \\\n",
       "BHATNAGAR SANJAY        -2604490.0                 15456290.0   \n",
       "\n",
       "                  total_stock_value  \n",
       "BHATNAGAR SANJAY                NaN  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q75, q25 = np.percentile(df[current_feature].dropna(), [75 ,25])\n",
    "iqr = q75 - q25\n",
    "\n",
    "# negative outliers:\n",
    "df[df[current_feature] < q25 - (iqr*1.5)] \n",
    "# positive outliers:\n",
    "#df[df[current_feature] > q75 + (iqr*1.5)][current_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "##### Fix & remove invalid columns\n",
    "\n",
    "`TOTAL`: aggregation of the existing datapoints and does not represent a person -> removed\n",
    "\n",
    "`THE TRAVEL AGENCY IN THE PARK`: not a real person -> removed\n",
    "\n",
    "`BHATNAGAR SANJAY`: columns shifted one to the left -> fixed\n",
    "\n",
    "`BELFER ROBERT`: columns shifted one to the left -> fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df.index != 'TOTAL']\n",
    "df = df[df.index != 'THE TRAVEL AGENCY IN THE PARK']\n",
    "\n",
    "df.loc['BELFER ROBERT', features_finance] = \\\n",
    "    [0, 0, 0, -102500, 0, 0, 0, 3285,\n",
    "     102500, 3285, 0, 44093, -44093, 0]\n",
    "df.loc['BHATNAGAR SANJAY', features_finance] = \\\n",
    "    [0, 0, 0, 0, 0, 0, 0, 137864, 0, 137864,\n",
    "     15456290, 2604490, -2604490, 15456290]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Log scaling\n",
    "\n",
    "While plotting the histograms of the features the highly skewed distribution of most of the features could be observed. As some models perform better or even require a normal distribution I applied log scaling to the features.\n",
    "The log scaling can easily be reverted using the exponential function no information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in features_list:\n",
    "    df[f] = [np.log(abs(v)) if v != 0 else 0 for v in df[f]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "Check for missing values per feature and compare the ratio of poi's within those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loan_advances</th>\n",
       "      <td>139</td>\n",
       "      <td>0.965278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director_fees</th>\n",
       "      <td>128</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <td>127</td>\n",
       "      <td>0.881944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferral_payments</th>\n",
       "      <td>105</td>\n",
       "      <td>0.729167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income</th>\n",
       "      <td>94</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>77</td>\n",
       "      <td>0.534722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>61</td>\n",
       "      <td>0.423611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_messages</th>\n",
       "      <td>58</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <td>58</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <td>58</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <td>58</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_messages</th>\n",
       "      <td>58</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>52</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>48</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>48</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>43</td>\n",
       "      <td>0.298611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock</th>\n",
       "      <td>34</td>\n",
       "      <td>0.236111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_payments</th>\n",
       "      <td>21</td>\n",
       "      <td>0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_stock_value</th>\n",
       "      <td>18</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_address</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poi</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1\n",
       "loan_advances              139  0.965278\n",
       "director_fees              128  0.888889\n",
       "restricted_stock_deferred  127  0.881944\n",
       "deferral_payments          105  0.729167\n",
       "deferred_income             94  0.652778\n",
       "long_term_incentive         77  0.534722\n",
       "bonus                       61  0.423611\n",
       "to_messages                 58  0.402778\n",
       "shared_receipt_with_poi     58  0.402778\n",
       "from_this_person_to_poi     58  0.402778\n",
       "from_poi_to_this_person     58  0.402778\n",
       "from_messages               58  0.402778\n",
       "other                       52  0.361111\n",
       "expenses                    48  0.333333\n",
       "salary                      48  0.333333\n",
       "exercised_stock_options     43  0.298611\n",
       "restricted_stock            34  0.236111\n",
       "total_payments              21  0.145833\n",
       "total_stock_value           18  0.125000\n",
       "email_address                0  0.000000\n",
       "poi                          0  0.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_null_value_ratio = (df.isnull().sum() / df.shape[0]).sort_values(ascending=False)\n",
    "df_null_values = (df.isnull().sum()).sort_values(ascending=False)\n",
    "frames = [df_null_values, df_null_value_ratio]\n",
    "pd.concat(frames, axis=1, join_axes=[df_null_values.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POIs:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "restricted_stock_deferred     0\n",
       "director_fees                 0\n",
       "loan_advances                 1\n",
       "deferral_payments             5\n",
       "deferred_income              11\n",
       "long_term_incentive          12\n",
       "exercised_stock_options      12\n",
       "to_messages                  14\n",
       "from_messages                14\n",
       "from_poi_to_this_person      14\n",
       "from_this_person_to_poi      14\n",
       "shared_receipt_with_poi      14\n",
       "bonus                        16\n",
       "restricted_stock             17\n",
       "salary                       17\n",
       "total_payments               18\n",
       "poi                          18\n",
       "other                        18\n",
       "email_address                18\n",
       "expenses                     18\n",
       "total_stock_value            18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of pois\n",
    "print 'Number of POIs:'\n",
    "df.loc[df['poi'] == True].count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deferred_income              0.220000\n",
       "loan_advances                0.200000\n",
       "other                        0.195652\n",
       "bonus                        0.192771\n",
       "expenses                     0.187500\n",
       "long_term_incentive          0.179104\n",
       "salary                       0.177083\n",
       "from_this_person_to_poi      0.162791\n",
       "to_messages                  0.162791\n",
       "from_messages                0.162791\n",
       "from_poi_to_this_person      0.162791\n",
       "shared_receipt_with_poi      0.162791\n",
       "restricted_stock             0.154545\n",
       "total_payments               0.146341\n",
       "total_stock_value            0.142857\n",
       "deferral_payments            0.128205\n",
       "poi                          0.125000\n",
       "email_address                0.125000\n",
       "exercised_stock_options      0.118812\n",
       "restricted_stock_deferred    0.000000\n",
       "director_fees                0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_non_null_values = (df.notnull().sum()).sort_values(ascending=False)\n",
    "df_number_pois = df.loc[df['poi'] == True].count().sort_values()\n",
    "(df_number_pois/df_non_null_values).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we only have very small sets of data for restricted_stock_deferred, director_fees, loan_advances and deferral_payments. Given that POI's only make up for 12.5% of our dataset the ratio of pois is higher than average in the bonus and expense categories.\n",
    "\n",
    "We got dataset that includes way more non-POI's than POI's, which needs to be considered when evaluating the classification algos. If an algo such as POI = False would be deployed the accuracy would already be at 86%, but precision at 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPUTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creation - Ratio of email & receipt sharing with pois\n",
    "\n",
    "There are three measurable interactions with POIs in the dataset: sending, recieving emails as well as sharing a reciept with a POI. Those interactions should be translated into ratios that compare them with the total number of each type of interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate ratio\n",
    "df['recieved_from_poi_ratio'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['sent_to_poi_ratio'] = df['from_this_person_to_poi'] / df['from_messages']\n",
    "df['shared_receipt_with_poi_ratio'] = df['shared_receipt_with_poi'] / df['to_messages']\n",
    "# add labels to df\n",
    "features_email_new = ['recieved_from_poi_ratio', 'sent_to_poi_ratio', 'shared_receipt_with_poi_ratio']\n",
    "features_all = features_list + features_email_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(features_email_new)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(df[features_email_new[i]].dropna(), bins=20)\n",
    "    plt.title(features_email_new[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(features_email_new)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(df[features_email_new[i]].dropna(), bins=20, log=True)\n",
    "    plt.title(features_email_new[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly introduced mail features are normal distributed while the reciept ratio is highly skewed. Given that log scaling won't change those distributions I will leave them as they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect for correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.corr().sort_values('poi', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,14))\n",
    "sns.heatmap(df.corr(), square=True, annot=True, fmt='.2f', linewidths=.5)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though loan advances has a very high correlation, given the percentage of missing values (97%) that feature won't be considered.\n",
    "The high correlation between the ratios and the absolute numbers of the email and receipt features are expected and when working on the best feature selection it should be evaluated which of those (number or ratio) should be considered. Using both would add the same feature twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Selection\n",
    "#### KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kbest = SelectKBest(f_classif, k='all')\n",
    "kbest.fit(Imputer(strategy='median').fit_transform(df[features_all]), df['poi'])\n",
    "features = [(f, round(s, 3), round(p, 4))\n",
    "            for f, s, p in zip(features_all, kbest.scores_, kbest.pvalues_)]\n",
    "features = DataFrame.from_records(features, index='feature',\n",
    "                                  columns=['feature', 'score', 'pvalue'])\n",
    "features.sort_values(by='score', ascending=False, inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr = SelectFpr(f_classif, alpha=0.05)\n",
    "fpr.fit(Imputer(strategy='median').fit_transform(df[features_all]), df['poi'])\n",
    "features = [(f, round(s, 3), round(p, 4))\n",
    "            for f, s, p in zip(features_all, fpr.scores_, fpr.pvalues_)\n",
    "            if p < 0.05]\n",
    "features = DataFrame.from_records(features, index='feature',\n",
    "                                  columns=['feature', 'score', 'pvalue'])\n",
    "features.sort_values(by='score', ascending=False, inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Select the pvalues below alpha based on a FPR test. FPR test stands for False Positive Rate test. It controls the total amount of false detections.\" (sklearn docs)\n",
    "\n",
    "Turns out that the FPR test returns the same values that would have been selected when applying the A/B testing industry standart of 5%. After removing the duplications caused by adding the ratios those are the selected features sorted by importance:\n",
    "\n",
    "| feature                 | score        | pvalue  |\n",
    "| ----------------------- |------------- | -----   |\n",
    "| sent_to_poi_ratio\t      | 13.433\t     | 0.0003  |\n",
    "| restricted_stock\t      | 8.769\t     | 0.0036  |\n",
    "| exercised_stock_options | 6.756\t     | 0.0103  |\n",
    "| recieved_from_poi_ratio | 6.005\t     | 0.0155  |\n",
    "| total_stock_value\t      | 5.866        | 0.0167  |\n",
    "| shared_receipt_with_poi | 5.678        | 0.0185  |\n",
    "| total_payments\t      | 4.872        | 0.0289  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get sklearn ready dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# features = scaler.fit_transform(features)\n",
    "print(df.isnull().sum())\n",
    "df.replace(0, np.NaN)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Get dictionary\n",
    "my_dataset = df.to_dict(orient='index')\n",
    "# print my_dataset\n",
    "\n",
    "#Select the labels\n",
    "my_feature_list = ['poi'] + ['sent_to_poi_ratio'] + ['restricted_stock'] + ['exercised_stock_options'] + ['recieved_from_poi_ratio'] + ['total_stock_value'] + ['shared_receipt_with_poi'] + ['total_payments']\n",
    "# print my_feature_list[1]\n",
    "\n",
    "\n",
    "# Format and splot to labels and features\n",
    "data = featureFormat(my_dataset, my_feature_list, remove_NaN=True, sort_keys = False)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "### Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_clf = KMeans(n_clusters=2, tol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "s_clf = SVC(kernel='rbf', C=1000,gamma = 0.0001,random_state = 42, class_weight = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(max_depth = 5,max_features = 'sqrt',n_estimators = 10, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The evaluation methods of choice are accuracy, precision and recall. The following function will return the values for each of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_clf(clf, features, labels, num_iters=1000, test_size=0.3):\n",
    "    print clf\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    first = True\n",
    "    for trial in range(num_iters):\n",
    "        features_train, features_test, labels_train, labels_test =\\\n",
    "            cross_validation.train_test_split(features, labels, test_size=test_size)\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        accuracy.append(accuracy_score(labels_test, predictions))\n",
    "        precision.append(precision_score(labels_test, predictions))\n",
    "        recall.append(recall_score(labels_test, predictions))\n",
    "        if trial % 10 == 0:\n",
    "            if first:\n",
    "                sys.stdout.write('\\nProcessing')\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "            first = False\n",
    "\n",
    "    print \"done.\\n\"\n",
    "    print \"precision: {}\".format(mean(precision))\n",
    "    print \"recall:    {}\".format(mean(recall))\n",
    "    return mean(precision), mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_clf(k_clf, features, labels)\n",
    "evaluate_clf(s_clf, features, labels)\n",
    "evaluate_clf(rf_clf, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Articles\n",
    "* A look at those involved in the Enron scandal, USA Today - http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm\n",
    "* The Immortal Life of the Enron E-mails, MIT Technology Review - https://www.technologyreview.com/s/515801/the-immortal-life-of-the-enron-e-mails/\n",
    "* Implementing a Weighted Majority Rule Ensemble Classifier in scikit-learn, Sebastian Raschka - http://sebastianraschka.com/Articles/2014_ensemble_classifier.html\n",
    "* Color Palettes in Seaborn, Chris Albon - http://chrisalbon.com/python/seaborn_color_palettes.html\n",
    "* Random Forests, Leo Breiman and Adele Cutler - http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "* Python sklearn.feature_selection.f_classif Examples - http://www.programcreek.com/python/example/85917/sklearn.feature_selection.f_classif\n",
    "* Handle missing data python - https://machinelearningmastery.com/handle-missing-data-python/\n",
    "\n",
    "### Cheatsheets\n",
    "* Markdown - https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\n",
    "* Pandas - https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf\n",
    "* Numpy - https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf\n",
    "\n",
    "### Documentation\n",
    "* Pipelining: chaining a PCA and a logistic regression, scikit learn - http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html\n",
    "* matplotlib.axes, matplotlib - http://matplotlib.org/api/axes_api.html\n",
    "* DataFrame quantiles, pandas - http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.quantile.html\n",
    "* Visualization, pandas - https://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "* pyplot, matplotlib - https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.hist.html\n",
    "* sort values, pandas - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html  \n",
    "* Working with missing data, pandas - https://pandas.pydata.org/pandas-docs/stable/missing_data.html\n",
    "* sklearn.feature_selection.SelectFpr - http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html\n",
    "\n",
    "### GitHub Repositories\n",
    "* EnsembleVoteClassifier, Sebastian Raschka - http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/\n",
    "* Grace Pehl: Identifying Persons of Interest from the Enron Corpus - https://github.com/grace-pehl/enron\n",
    "* brandjamie: Marchine Learning with the enron emails dataset - https://github.com/brandjamie/udacity_enron\n",
    "* Daria ALekseeva: Enron Dataset - https://github.com/DariaAlekseeva/Enron_Dataset\n",
    "* watanabe8760: uda-da-p5-enron-fraud-detection - https://github.com/watanabe8760/uda-da-p5-enron-fraud-detection\n",
    "* Mayukh Sobo: Enron Fraud https://github.com/MayukhSobo/EnronFraud \n",
    "\n",
    "### Q&A pages\n",
    "* Pandas Replacement for .ix, Stack Overflow  - https://stackoverflow.com/questions/43838999/pandas-replacement-for-ix\n",
    "* Sci-kit and Regression Summary, Stack Overflow - http://stackoverflow.com/questions/26319259/sci-kit-and-regression-summary\n",
    "* How to obtain True Positive, True Negative, False Positive and False Negative, Stack Overflow - https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "* Why do we need to normalize data before analysis, Cross Validated - http://stats.stackexchange.com/questions/69157/why-do-we-need-to-normalize-data-before-analysis\n",
    "* Perform feature normalization before or within model validation?, Cross Validated - http://stats.stackexchange.com/questions/77350/perform-feature-normalization-before-or-within-model-validation\n",
    "* How should the interquartile range be calculated in Python?, Stack Overflow - http://stackoverflow.com/questions/27472330/how-should-the-interquartile-range-be-calculated-in-python\n",
    "* scikit learn svc coef0 parameter range, Stack Overflow - http://stackoverflow.com/questions/21390570/scikit-learn-svc-coef0-parameter-range\n",
    "* What is a good range of values for the svm.SVC() hyperparameters to be explored via GridSearchCV()?, Stack Overflow - http://stackoverflow.com/questions/26337403/what-is-a-good-range-of-values-for-the-svm-svc-hyperparameters-to-be-explored\n",
    "* Imputation before or after splitting into train and test?, Cross Validated - http://stats.stackexchange.com/questions/95083/imputation-before-or-after-splitting-into-train-and-test\n",
    "* Is there a rule-of-thumb for how to divide a dataset into training and validation sets?, Stack Overflow - http://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio\n",
    "* What is the difference between test set and validation set?, Cross Validated - http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set\n",
    "* Python - What is exactly sklearn.pipeline.Pipeline?, Stack Overflow - http://stackoverflow.com/questions/33091376/python-what-is-exactly-sklearn-pipeline-pipeline\n",
    "* How can I use a custom feature selection function in scikit-learn's pipeline, Stack Overflow - http://stackoverflow.com/questions/25250654/how-can-i-use-a-custom-feature-selection-function-in-scikit-learns-pipeline\n",
    "* Seaborn distplot y-axis normalisation wrong ticklabels, Stack Overflow - http://stackoverflow.com/questions/32274865/seaborn-distplot-y-axis-normalisation-wrong-ticklabels\n",
    "* How to save a Seaborn plot into a file, Stack Overflow - http://stackoverflow.com/questions/32244753/how-to-save-a-seaborn-plot-into-a-file\n",
    "* Seaborn plots not showing up, Stack Overflow - https://stackoverflow.com/questions/26597116/seaborn-plots-not-showing-up\n",
    "* Select rows from a dataframe, Stack Overflow - https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas\n",
    "\n",
    "### Tools\n",
    "* Markdown Tables Generator - http://www.tablesgenerator.com/markdown_tables\n",
    "* JSON pretty print - http://jsonprettyprint.com\n",
    "\n",
    "### Wikipedia\n",
    "* Enron scandal - https://en.wikipedia.org/wiki/Enron_scandal\n",
    "* Boxplots - https://en.wikipedia.org/wiki/Box_plot\n",
    "* Interquartile range - https://en.wikipedia.org/wiki/Interquartile_range\n",
    "* False positive rate - https://en.wikipedia.org/wiki/False_positive_rate\n",
    "* False discovery rate - https://en.wikipedia.org/wiki/False_discovery_rate\n",
    "* Precision and recall - https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
